{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:/EEG DATA/train_ordered/02_tcp_le/Formatted/00000002/00000002_s005.txt'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"E:\\EEG DATA\\train_ordered\\02_tcp_le\\Text_data\\text_meta.csv\")\n",
    "data.head()\n",
    "data['edf_location'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2a2398d5b44ff2a9e66d9fd922370c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=181), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Error\n",
      "Encoding Error\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#Function to retrieve the medical reports\n",
    "def get_report(link):\n",
    "    try:\n",
    "        with open(link, 'r') as myfile:\n",
    "            txt = myfile.read().replace('\\n', '')\n",
    "    except:\n",
    "        print(\"Encoding Error\")\n",
    "        return \"None\"\n",
    "    myfile.close()\n",
    "    return txt\n",
    "\n",
    "#data['edf_location'] = data['edf_location'].progress_apply(lambda txt: str(txt))\n",
    "data['report'] = data['edf_location'].progress_apply(lambda link: get_report(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just two files caused a great deal of trouble!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        edf_location  label  \\\n",
      "0  E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...      0   \n",
      "1  E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...      1   \n",
      "2  E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...      0   \n",
      "3  E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...      0   \n",
      "4  E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...      0   \n",
      "\n",
      "                                              report  \n",
      "0  CLINICAL HISTORY: This is a 55-year-old, right...  \n",
      "1  CLINICAL HISTORY: This is a 27-year-old woman ...  \n",
      "2  CLINICAL HISTORY: This is a 23-year-old male w...  \n",
      "3  CLINICAL HISTORY: 56 year old male with a smal...  \n",
      "4  CLINICAL HISTORY: 56 year old male with a smal...  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "data['report'] = data['report'].apply(lambda txt: str(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords from the NLTK corpus:  {'have', 'such', 'our', 'the', 'into', 'until', \"isn't\", 'herself', 'out', 'has', 'only', 'aren', 'some', 'nor', 'just', \"doesn't\", 'does', 'during', \"you'd\", 'itself', 'be', 'him', 'to', \"hasn't\", 'her', 'over', 'any', 'didn', 'mustn', \"aren't\", 'don', 'where', 'theirs', 'as', 'under', 'is', 'off', \"you'll\", 'more', 'wasn', 'o', 'himself', 'because', 'these', 'll', 'other', 'was', 'about', 'how', 'ours', 'both', \"shouldn't\", 'hadn', 'won', 'ma', \"it's\", 'further', 'couldn', 'did', 'had', 'at', 'than', 'again', 'a', \"weren't\", 'can', 'do', 'all', 'themselves', 'wouldn', 'having', 'shouldn', 'each', 'been', 'few', 'what', 've', \"should've\", 'it', 't', 'whom', 'too', 'that', 'being', 'while', 'i', \"that'll\", \"hadn't\", 'yourselves', 'most', 'from', \"wouldn't\", 'and', 'we', 'y', 'she', 'ain', 'above', 're', 'with', 'needn', \"needn't\", \"you've\", 'when', 'shan', 'up', 'd', 'haven', 'for', \"haven't\", 'you', 'so', 'then', 'below', 'my', 'of', 'or', 'down', 'weren', 'me', \"mustn't\", 'yourself', 'no', \"won't\", 'their', 'its', \"shan't\", 'there', 'but', 'are', 'mightn', 'will', 'before', 'them', \"she's\", 'very', 'should', 'if', 'doing', 'am', 'on', 'your', 'after', 'same', 'own', 'myself', 'hers', 'here', \"wasn't\", 'not', 'hasn', 'in', 'he', 'through', \"didn't\", 'why', 'isn', \"mightn't\", 'they', 'were', 'by', 's', 'who', \"don't\", 'those', 'm', \"you're\", 'between', 'against', 'yours', 'his', 'once', 'an', \"couldn't\", 'which', 'now', 'this', 'doesn', 'ourselves'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92559e6165814e2ab50c17d5b7f6730e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=181), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(\"Stopwords from the NLTK corpus: \", stop_words)\n",
    "\n",
    "def text_process(text):\n",
    "    #String punctuation provides all the necessary checks\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "data['report'] = data['report'].progress_apply(lambda txt: text_process(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8eac54d61f746038eae73c10e5f0f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=181), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def join_back(txt):\n",
    "    new = ''\n",
    "    for x in txt:\n",
    "        new += ''.join(x)+\" \"\n",
    "    return new\n",
    "\n",
    "data['report'] = data['report'].progress_apply(lambda txt: join_back(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edf_location</th>\n",
       "      <th>label</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...</td>\n",
       "      <td>0</td>\n",
       "      <td>CLINICAL HISTORY 55yearold righthanded woman C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...</td>\n",
       "      <td>1</td>\n",
       "      <td>CLINICAL HISTORY 27yearold woman focal scleros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...</td>\n",
       "      <td>0</td>\n",
       "      <td>CLINICAL HISTORY 23yearold male history seizur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...</td>\n",
       "      <td>0</td>\n",
       "      <td>CLINICAL HISTORY 56 year old male small lacuna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...</td>\n",
       "      <td>0</td>\n",
       "      <td>CLINICAL HISTORY 56 year old male small lacuna...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        edf_location  label  \\\n",
       "0  E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...      0   \n",
       "1  E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...      1   \n",
       "2  E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...      0   \n",
       "3  E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...      0   \n",
       "4  E:/EEG DATA/train_ordered/02_tcp_le/Formatted/...      0   \n",
       "\n",
       "                                              report  \n",
       "0  CLINICAL HISTORY 55yearold righthanded woman C...  \n",
       "1  CLINICAL HISTORY 27yearold woman focal scleros...  \n",
       "2  CLINICAL HISTORY 23yearold male history seizur...  \n",
       "3  CLINICAL HISTORY 56 year old male small lacuna...  \n",
       "4  CLINICAL HISTORY 56 year old male small lacuna...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"E:\\EEG DATA\\train_ordered\\02_tcp_le\\Text_data\\text_meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Term Matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#Tokenizing and Cleaning - again - satisfaction\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (2,2),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(data['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_counts, data['label'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 17362, 32)         555584    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               186400    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 742,185\n",
      "Trainable params: 742,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embd_size=32\n",
    "vocab_size = text_counts.shape[1]\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size, embd_size, input_length = text_counts.shape[1]))\n",
    "model.add(LSTM(200))\n",
    "#Add dropout - Regularization\n",
    "#Add ensemble of engineered features\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "144/144 [==============================] - ETA: 3:19 - loss: 0.6931 - acc: 0.800 - ETA: 3:13 - loss: 0.6958 - acc: 0.600 - ETA: 3:11 - loss: 0.6930 - acc: 0.600 - ETA: 3:06 - loss: 0.6953 - acc: 0.550 - ETA: 3:01 - loss: 0.7000 - acc: 0.480 - ETA: 2:55 - loss: 0.6956 - acc: 0.533 - ETA: 2:48 - loss: 0.6925 - acc: 0.571 - ETA: 2:41 - loss: 0.6935 - acc: 0.550 - ETA: 2:35 - loss: 0.6927 - acc: 0.555 - ETA: 2:30 - loss: 0.6920 - acc: 0.560 - ETA: 2:26 - loss: 0.6947 - acc: 0.527 - ETA: 2:22 - loss: 0.6939 - acc: 0.533 - ETA: 2:18 - loss: 0.6919 - acc: 0.553 - ETA: 2:14 - loss: 0.6927 - acc: 0.542 - ETA: 2:10 - loss: 0.6934 - acc: 0.533 - ETA: 2:08 - loss: 0.6929 - acc: 0.537 - ETA: 2:03 - loss: 0.6924 - acc: 0.541 - ETA: 1:56 - loss: 0.6908 - acc: 0.555 - ETA: 1:48 - loss: 0.6892 - acc: 0.568 - ETA: 1:40 - loss: 0.6863 - acc: 0.590 - ETA: 1:31 - loss: 0.6846 - acc: 0.600 - ETA: 1:22 - loss: 0.6826 - acc: 0.609 - ETA: 1:12 - loss: 0.6823 - acc: 0.608 - ETA: 1:04 - loss: 0.6793 - acc: 0.616 - ETA: 53s - loss: 0.6860 - acc: 0.600 - ETA: 42s - loss: 0.6921 - acc: 0.58 - ETA: 28s - loss: 0.6914 - acc: 0.58 - ETA: 12s - loss: 0.6907 - acc: 0.58 - 476s 3s/step - loss: 0.6911 - acc: 0.5833\n",
      "Epoch 2/5\n",
      "144/144 [==============================] - ETA: 15:30 - loss: 0.6749 - acc: 0.60 - ETA: 14:53 - loss: 0.7014 - acc: 0.50 - ETA: 14:37 - loss: 0.7092 - acc: 0.46 - ETA: 14:15 - loss: 0.7006 - acc: 0.50 - ETA: 13:46 - loss: 0.6879 - acc: 0.56 - ETA: 13:53 - loss: 0.6863 - acc: 0.56 - ETA: 13:53 - loss: 0.6852 - acc: 0.57 - ETA: 13:38 - loss: 0.6748 - acc: 0.62 - ETA: 12:57 - loss: 0.6796 - acc: 0.60 - ETA: 12:34 - loss: 0.6835 - acc: 0.58 - ETA: 12:18 - loss: 0.6793 - acc: 0.60 - ETA: 11:50 - loss: 0.6792 - acc: 0.60 - ETA: 11:21 - loss: 0.6758 - acc: 0.61 - ETA: 10:50 - loss: 0.6823 - acc: 0.58 - ETA: 10:17 - loss: 0.6819 - acc: 0.58 - ETA: 9:38 - loss: 0.6788 - acc: 0.6000 - ETA: 8:55 - loss: 0.6811 - acc: 0.588 - ETA: 8:12 - loss: 0.6808 - acc: 0.588 - ETA: 7:30 - loss: 0.6832 - acc: 0.578 - ETA: 6:47 - loss: 0.6828 - acc: 0.580 - ETA: 6:03 - loss: 0.6847 - acc: 0.571 - ETA: 5:18 - loss: 0.6822 - acc: 0.581 - ETA: 4:31 - loss: 0.6840 - acc: 0.573 - ETA: 3:44 - loss: 0.6836 - acc: 0.575 - ETA: 2:58 - loss: 0.6833 - acc: 0.576 - ETA: 2:12 - loss: 0.6830 - acc: 0.576 - ETA: 1:24 - loss: 0.6828 - acc: 0.577 - ETA: 37s - loss: 0.6843 - acc: 0.571 - 1366s 9s/step - loss: 0.6814 - acc: 0.5833\n",
      "Epoch 3/5\n",
      "144/144 [==============================] - ETA: 24:49 - loss: 0.6714 - acc: 0.60 - ETA: 22:49 - loss: 0.6984 - acc: 0.50 - ETA: 22:32 - loss: 0.6907 - acc: 0.53 - ETA: 21:44 - loss: 0.6592 - acc: 0.65 - ETA: 20:46 - loss: 0.6385 - acc: 0.72 - ETA: 20:00 - loss: 0.6557 - acc: 0.66 - ETA: 19:18 - loss: 0.6475 - acc: 0.68 - ETA: 18:31 - loss: 0.6718 - acc: 0.62 - ETA: 17:44 - loss: 0.6719 - acc: 0.62 - ETA: 16:52 - loss: 0.6636 - acc: 0.64 - ETA: 16:05 - loss: 0.6564 - acc: 0.65 - ETA: 15:16 - loss: 0.6579 - acc: 0.65 - ETA: 14:27 - loss: 0.6591 - acc: 0.64 - ETA: 13:38 - loss: 0.6604 - acc: 0.64 - ETA: 12:47 - loss: 0.6615 - acc: 0.64 - ETA: 11:54 - loss: 0.6699 - acc: 0.62 - ETA: 10:58 - loss: 0.6766 - acc: 0.61 - ETA: 10:03 - loss: 0.6867 - acc: 0.58 - ETA: 9:07 - loss: 0.6902 - acc: 0.5789 - ETA: 8:12 - loss: 0.6860 - acc: 0.590 - ETA: 7:17 - loss: 0.6825 - acc: 0.600 - ETA: 6:21 - loss: 0.6821 - acc: 0.600 - ETA: 5:25 - loss: 0.6861 - acc: 0.582 - ETA: 4:29 - loss: 0.6899 - acc: 0.566 - ETA: 3:33 - loss: 0.6857 - acc: 0.584 - ETA: 2:37 - loss: 0.6870 - acc: 0.576 - ETA: 1:41 - loss: 0.6851 - acc: 0.585 - ETA: 45s - loss: 0.6849 - acc: 0.585 - 1640s 11s/step - loss: 0.6853 - acc: 0.5833\n",
      "Epoch 4/5\n",
      "144/144 [==============================] - ETA: 25:50 - loss: 0.6784 - acc: 0.60 - ETA: 25:25 - loss: 0.6595 - acc: 0.70 - ETA: 24:20 - loss: 0.6404 - acc: 0.80 - ETA: 23:40 - loss: 0.6495 - acc: 0.75 - ETA: 22:43 - loss: 0.6463 - acc: 0.76 - ETA: 21:51 - loss: 0.6515 - acc: 0.73 - ETA: 21:01 - loss: 0.6552 - acc: 0.71 - ETA: 20:04 - loss: 0.6521 - acc: 0.72 - ETA: 19:10 - loss: 0.6547 - acc: 0.71 - ETA: 18:10 - loss: 0.6620 - acc: 0.68 - ETA: 17:17 - loss: 0.6534 - acc: 0.70 - ETA: 16:21 - loss: 0.6504 - acc: 0.71 - ETA: 15:23 - loss: 0.6522 - acc: 0.70 - ETA: 14:24 - loss: 0.6588 - acc: 0.68 - ETA: 13:26 - loss: 0.6597 - acc: 0.68 - ETA: 12:29 - loss: 0.6557 - acc: 0.68 - ETA: 11:31 - loss: 0.6567 - acc: 0.68 - ETA: 10:32 - loss: 0.6576 - acc: 0.67 - ETA: 9:34 - loss: 0.6692 - acc: 0.6526 - ETA: 8:36 - loss: 0.6788 - acc: 0.630 - ETA: 7:38 - loss: 0.6824 - acc: 0.619 - ETA: 6:39 - loss: 0.6820 - acc: 0.618 - ETA: 5:41 - loss: 0.6762 - acc: 0.634 - ETA: 4:42 - loss: 0.6761 - acc: 0.633 - ETA: 3:43 - loss: 0.6830 - acc: 0.608 - ETA: 2:45 - loss: 0.6846 - acc: 0.600 - ETA: 1:46 - loss: 0.6843 - acc: 0.600 - ETA: 47s - loss: 0.6853 - acc: 0.592 - 1711s 12s/step - loss: 0.6873 - acc: 0.5833\n",
      "Epoch 5/5\n",
      "144/144 [==============================] - ETA: 26:13 - loss: 0.6776 - acc: 0.60 - ETA: 25:15 - loss: 0.6755 - acc: 0.60 - ETA: 24:47 - loss: 0.6638 - acc: 0.66 - ETA: 23:58 - loss: 0.6582 - acc: 0.70 - ETA: 23:03 - loss: 0.6624 - acc: 0.68 - ETA: 22:16 - loss: 0.6713 - acc: 0.63 - ETA: 21:19 - loss: 0.6673 - acc: 0.65 - ETA: 20:24 - loss: 0.6642 - acc: 0.67 - ETA: 19:30 - loss: 0.6699 - acc: 0.64 - ETA: 18:36 - loss: 0.6782 - acc: 0.60 - ETA: 17:42 - loss: 0.6717 - acc: 0.63 - ETA: 16:40 - loss: 0.6692 - acc: 0.65 - ETA: 15:43 - loss: 0.6700 - acc: 0.64 - ETA: 14:45 - loss: 0.6706 - acc: 0.64 - ETA: 13:44 - loss: 0.6737 - acc: 0.62 - ETA: 12:42 - loss: 0.6740 - acc: 0.62 - ETA: 11:43 - loss: 0.6742 - acc: 0.62 - ETA: 10:45 - loss: 0.6744 - acc: 0.62 - ETA: 9:45 - loss: 0.6746 - acc: 0.6211 - ETA: 8:46 - loss: 0.6748 - acc: 0.620 - ETA: 7:44 - loss: 0.6764 - acc: 0.609 - ETA: 6:45 - loss: 0.6784 - acc: 0.600 - ETA: 5:45 - loss: 0.6783 - acc: 0.600 - ETA: 4:46 - loss: 0.6801 - acc: 0.591 - ETA: 3:47 - loss: 0.6800 - acc: 0.592 - ETA: 2:47 - loss: 0.6799 - acc: 0.592 - ETA: 1:47 - loss: 0.6782 - acc: 0.600 - ETA: 47s - loss: 0.6812 - acc: 0.585 - 1738s 12s/step - loss: 0.6817 - acc: 0.5833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de0fc35a20>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.86%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
